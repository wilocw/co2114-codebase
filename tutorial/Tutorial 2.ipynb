{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c957d1-d150-4f67-b231-24f5b8b349a8",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/wilocw/co2114-codebase/2024/static/0/uol_banner_red.png)\r\n",
    "\n",
    "# CO2114<br />Foundations in Artificial Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fc7507-fb95-4c0b-b234-5fd91384601f",
   "metadata": {},
   "source": [
    "# Tutorial 2 - Agent Design \n",
    "\n",
    "This tutorial will cover the design of rational agents using the _\"performance, environment, actuators, sensors\"_ (PEAS) framework. The aim of this tutorial is for you to understand how to specify the task environment for a problem and develop the building blocks for agent design. We will also look at the implementation of a simple reflex program.\n",
    "\n",
    "This tutorial will cover:\n",
    "- Describing task environments with PEAS\n",
    "- Characterising the environment\n",
    "- Implementing a simple reflex agent program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7836f5-ef44-4905-ab85-6d17cbfd6477",
   "metadata": {},
   "source": [
    "## Recap: Task Environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e387b351-2337-47d8-a8af-bf769cc6fd92",
   "metadata": {},
   "source": [
    "The _task environment_ is a description of the specific _problem_ an agent is designed to solve. Understanding the task environment gives scope by which to implement an agent's function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cb7733-07b6-4c3c-819c-ceee8803336c",
   "metadata": {},
   "source": [
    "### PEAS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cf56fa-f582-473f-97cc-046916cd3d44",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Recall our previous example of the **self-driving car**. Here we have a set of criteria to measure the performance of agent; the environment in which the car operates; the actuators that the car can control; and the sensors that describe how it percieves its current state and the state of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6873bda2-5439-4964-b796-f069d79eed02",
   "metadata": {},
   "source": [
    "| Performance | Environment | Actuators | Sensors |\n",
    "|-------------|-------------|-----------|---------|\n",
    "| Safety, speed, legality, comfort, profit ...| Roads, traffic, pedestrians, customer, weather ... | Steering, acceleration, braking, signalling | Cameras, speedometer, GPS, engine sensors, accelerometers ... |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96e4eba-d0f6-4fef-aaba-76a78b83300b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "We define the constituent parts of the task environment as follows:\n",
    "\n",
    "##### Performance\n",
    " > The performance measure defines the success criteria of an agent. It describes the objective function(s) by which the agent's actions are evaluated, and determines whether the agent is achieving its goals.\n",
    " >\n",
    " > Performance measures should be _clearly defined_, _measurable_, and _relevant_ to the task at hand.\n",
    "\n",
    "##### Environment\n",
    "> The environment is the _external_ context in which an agent is operating. There are different classifications of environment based on, e.g., observability and stochasticity.\n",
    ">\n",
    "> The environment should include _all factors_ that influence the agent's _perception_ and _actions_.\n",
    "\n",
    "##### Actuators\n",
    "> Actuators are the mechanisms through which an agent interacts with its environment. The enable the agent to perform actions and influence the state of the environment.\n",
    ">\n",
    "> The choice of actuators is determined by the actions that are _required_ and the agents _capabilities_.\n",
    "\n",
    "##### Sensors\n",
    "> Sensors are the means by which an agent is able to perceive its environment. Sensors provide information to the agent about the state of the environment.\n",
    ">\n",
    "> Choosing sensors depends on the information that is required by the agent to both _execute its actions_ and evaluate its _performance_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfb52a0-86e5-4948-a50d-86f37443598b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#####\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b741dd2c-811e-4be8-975b-8af0a63c779c",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "For the following four example problems, describe the task environment for an agent using the PEAS framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a1a84a-1496-4aa2-8586-2a640cd1ec0f",
   "metadata": {},
   "source": [
    "### a) Playing a tennis match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d2efdf-d5b7-4676-aff3-f5f8c406c94c",
   "metadata": {},
   "source": [
    "##### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2a0e6a-c3d7-41d5-999f-d34d9eb588fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f260b983-5a1f-459d-ae12-f5f9d94d3f91",
   "metadata": {},
   "source": [
    "##### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa734e-622d-4c4b-9db7-c13780f4f75a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e43fdee3-fea8-4bbd-9516-148d058ae38b",
   "metadata": {},
   "source": [
    "##### Actuators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d9e3c3-3302-43e8-bd3d-c2668cac5d26",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe3b8f7d-2d6b-49f1-9abd-4a5a17ea5f46",
   "metadata": {},
   "source": [
    "##### Sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5292de2-5b57-4183-af0c-b82b6bfbdae7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2af4fa25-faa4-43f5-8315-7a98fc7fe7fe",
   "metadata": {},
   "source": [
    "### b) Playing a football match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f54b76-77eb-4ce9-aeab-4ccd09a744de",
   "metadata": {},
   "source": [
    "##### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d370ba2-bce5-4763-9957-599de58c31d6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e35ec8c-fd3e-4183-9d92-74eec7353ce0",
   "metadata": {},
   "source": [
    "##### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eea5d0e-9fea-433c-98c6-9833fc28a4f4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0319ee1-73e0-4ad1-9cef-9ba6c90ff64b",
   "metadata": {},
   "source": [
    "##### Actuators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c6be91-afb8-4b36-93b4-7476f1575ec4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f46287b5-4d8f-495c-a67c-96b7bc98fc12",
   "metadata": {},
   "source": [
    "##### Sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667743c4-77d2-4d7b-82ed-1423b6521516",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12b2a16f-68ec-47a4-9641-a96464b236dd",
   "metadata": {},
   "source": [
    "### c) Recommending films to watch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad757d-fad8-41ae-b283-d089dcc939f6",
   "metadata": {},
   "source": [
    "##### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d96d3a-83a3-4bbe-8b6f-664bcfe6bede",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4680f8c5-2703-48f5-8827-c2cb54ce4263",
   "metadata": {},
   "source": [
    "##### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19015f8-974a-4054-b0d9-d70a16a02fb6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f4ec097-9c67-4105-9e9f-f4e13a2020fc",
   "metadata": {},
   "source": [
    "##### Actuators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f685bfb6-f00e-470f-b0e3-73b36310147a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f40a418-87c7-4091-834b-2eb91a57d5ca",
   "metadata": {},
   "source": [
    "##### Sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a80969-7c6f-4e68-9a54-7b6bf43d2058",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dbd116b0-d7af-436c-9ee1-9dccd5d5f72b",
   "metadata": {},
   "source": [
    "### d) Bidding on an item at an auction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23d9f1-2772-4e87-ab92-dcc97bbc0081",
   "metadata": {},
   "source": [
    "##### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c169745-2917-4031-84de-517c8a810092",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0a96403-0453-4c04-974d-ed27116c7620",
   "metadata": {},
   "source": [
    "##### Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a83a3f9-1542-422b-94f2-9e45fbf72641",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "86ebcc91-bff2-4a04-a8e9-683a06187882",
   "metadata": {},
   "source": [
    "##### Actuators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb28d4b4-1e58-4fa0-80f0-ff9168212378",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1239ab28-aefe-4bf0-810b-b3b8ac3fd7b4",
   "metadata": {},
   "source": [
    "##### Sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d614bed-69a9-4c0e-95d5-f11534b38984",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95e2086d-96cd-4f79-840d-530cca9b5404",
   "metadata": {},
   "source": [
    "#####\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d5e93f-965a-43b8-a018-60a7f0cf4356",
   "metadata": {},
   "source": [
    "## Recap: Environment Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5272c48-32de-4d4c-992f-d7153876717b",
   "metadata": {},
   "source": [
    "##### Fully vs Partially Observable\n",
    "> An environment can either be _fully observable_, where an agent's sensors give it access to the complete state of the environment; _partially observable_, where the agent has an incomplete view of the environment; or even _unobservable_, if the agent has no sensors at all.\n",
    "\n",
    "##### Single- vs Multi-agent\n",
    "> A task environment may host a _single agent_; or _multiple interacting agents_. Multi-agent systems can be further categorised as _competitive_, where their performance measures conflict, or _co-operative_, where they are working towards the same outcome.\n",
    "\n",
    "##### Deterministic vs Stochastic\n",
    "> An environment is _deterministic_ if the next state is completely determined by the current state. This is not the case when it is affected by some random effect, in which case it is _stochastic_.\n",
    "\n",
    "##### Episodic vs Sequential\n",
    "> A task environment is episodic if each action by the agent is independent of the previous. It is considered sequential if the current action has influence on future actions.\n",
    "\n",
    "##### Static vs Dynamic\n",
    "> If the environment does not change between an agent's percept and its action, it is considered _static_. An environment that is changing while the agent makes its decision is _dynamic_.\n",
    "\n",
    "##### Discrete vs Continuous\n",
    "> If time, percepts or actions are all discrete, i.e. can be divided into individual and distinct steps, then it the environment is (fully) _discrete_. One of these sequences may be continuous, in which case, thee the environment is (partially) continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e611e9-a812-4f04-a796-9344f756a129",
   "metadata": {},
   "source": [
    "#####\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f0801-7793-4613-a6c8-01b8727071d8",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b729b5-5b61-47fd-a31a-26ce1258476d",
   "metadata": {},
   "source": [
    "For the following examples, characterise the task environments. Indicate your reasons for your choice of characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84890e92-571d-4017-9b32-abbfdbbbc653",
   "metadata": {},
   "source": [
    "### a) Playing a Football Match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bb3350-1021-4903-90bb-3350ca7c94c4",
   "metadata": {},
   "source": [
    "**Fully or Partially Observable**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da7c43a-373b-4fd5-b3df-2c02843df002",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a603579-6474-423a-bd4d-03ab0a1c8341",
   "metadata": {},
   "source": [
    "**Single- or Multi-Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ece55a5-ede3-4e99-bf48-5e68f15326a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd3cd907-b7ad-4485-8d85-08c907dd824a",
   "metadata": {},
   "source": [
    "**Deterministic or Stochastic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5c1e79-79b1-47c0-acdb-10e9e9b8a3e3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df4f5bca-e42b-4395-95de-6404316947c2",
   "metadata": {},
   "source": [
    "**Episodic or Sequential**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ea6449-4a20-4519-8f84-6ce1610866ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "813833b8-3ef3-46d4-a046-0849423b81eb",
   "metadata": {},
   "source": [
    "**Static or Dynamic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80ce1f7-68fa-4215-b7cc-62a2925ffb8a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a229e28-abd7-4542-b4d2-602b76d46e08",
   "metadata": {},
   "source": [
    "**Discrete or Continuous**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4a7c22-13ef-43f4-9866-d315f9bdf0fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62000244-5cd1-4697-ae29-4d14f6f5c407",
   "metadata": {},
   "source": [
    "### b) Practicing tennis against a wall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf3ec04-1c1b-4bdd-962b-1ad5b38bd85c",
   "metadata": {},
   "source": [
    "**Fully or Partially Observable**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcd6d36-4ccf-436d-8d50-99558aff51f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d973d25-987c-4de7-8fe4-e1aa349dadf9",
   "metadata": {},
   "source": [
    "**Single- or Multi-Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e2a0f2-e238-4c74-acc0-473ab366d96e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09e02824-b82a-474a-bb91-f630363f6e61",
   "metadata": {},
   "source": [
    "**Deterministic or Stochastic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a4b655-388b-4845-9d03-e833f211540f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59771026-4576-4f3c-b785-e006da2ed5aa",
   "metadata": {},
   "source": [
    "**Episodic or Sequential**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d512aa8-99c0-414d-b874-4924e5dc8639",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8bb32ff2-7fc8-469d-91ef-e28ad26f4334",
   "metadata": {},
   "source": [
    "**Static or Dynamic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20388bb4-f6f7-4e07-af83-7ccc82ade6ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37eafb71-e723-4374-be79-b0d659ec04f1",
   "metadata": {},
   "source": [
    "**Discrete or Continuous**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf45bb-3985-44f3-8749-f47b933538c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be649174-4d3b-469d-aa0e-edf1a266cb4f",
   "metadata": {},
   "source": [
    "### c) Medical diagnosis system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f873f366-6cb2-4e41-b640-1333fca5c0e0",
   "metadata": {},
   "source": [
    "**Fully or Partially Observable**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef83ca44-a81e-4078-99c7-881429a3f7dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d24654a7-2cd1-4fba-b898-e58ca654ce9c",
   "metadata": {},
   "source": [
    "**Single- or Multi-Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d79c15-c899-490f-a294-2638c4c91bc4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b5932f5-306e-4d8a-b532-f67f8a752b4f",
   "metadata": {},
   "source": [
    "**Deterministic or Stochastic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2b800e-1b9f-4650-b0f1-65de39ef7dd7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb5d124a-5aeb-4427-a3eb-d2351652e104",
   "metadata": {},
   "source": [
    "**Episodic or Sequential**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1f663c-633e-42a6-be9a-cf77d2619cb2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9d8c0f2-1dd7-404c-8c2a-fae8c5831b18",
   "metadata": {},
   "source": [
    "**Static or Dynamic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab86f10-a511-4d61-8e1d-5f36059eccf7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d68c9462-daac-43dd-bd6a-9f1bf16a2d14",
   "metadata": {},
   "source": [
    "**Discrete or Continuous**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474011ba-b5e2-4a26-a7b9-d081ed76e5b5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "748b92a8-58ad-4766-b588-e20b3a1f075d",
   "metadata": {},
   "source": [
    "### d) Self-driving car"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8515b1-ca33-485d-96a9-cc3332962d3d",
   "metadata": {},
   "source": [
    "**Fully or Partially Observable**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d205a277-a0f1-4857-8e01-c27ba5ec9e1c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2e7fd83-cec8-474a-a8d9-71140c3d3048",
   "metadata": {},
   "source": [
    "**Single- or Multi-Agent**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce200db8-4daf-47c1-8223-139ef943d3d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3aafed26-be19-427a-87c1-f2bf327ba16b",
   "metadata": {},
   "source": [
    "**Deterministic or Stochastic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33cd54c-ecaf-459e-8738-a212f49df0f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08d21058-75f1-43c6-bae4-6806ac77587f",
   "metadata": {},
   "source": [
    "**Episodic or Sequential**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291c2ed0-37cf-4b20-80d5-66b424d00379",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad9bd1d3-0e9c-44e6-9177-e92474fb6428",
   "metadata": {},
   "source": [
    "**Static or Dynamic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26f093a-7cf4-4d32-84af-77d9304c5e02",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e5887f1-9fad-491d-b159-02a824e55015",
   "metadata": {},
   "source": [
    "**Discrete or Continuous**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f739e-7cb5-4e63-918c-90f789a09f88",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cda4ff29-ada8-4a67-9ead-88917b337823",
   "metadata": {},
   "source": [
    "#####\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea635a6e-4f49-418c-b1d7-6afa4839df6f",
   "metadata": {},
   "source": [
    "## Recap: Reflex Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e280a5-57ec-43ad-a25a-e0dd51524649",
   "metadata": {},
   "source": [
    "> A **rational agent** is an _agent_ that selects an _action_ to maximise its **performance measure**, given the **evidence** it has _perceived_ and any built-in **knowledge** it has."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c956ff-bf94-41ef-8d09-9789b30ff277",
   "metadata": {},
   "source": [
    "<img width=450 src=\"https://raw.githubusercontent.com/wilocw/co2114-codebase/2024/static/2/rational_agent_diagram.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd6758b-622f-40f5-80a4-aa3d4ad5670f",
   "metadata": {},
   "source": [
    "When implementing an agent, we first consider the environment.\n",
    "\n",
    "An environment should provide _percepts_ to an agent, and excute its action, updating the environment based on this.\n",
    "\n",
    "We also consider the environment as an event-driven simulation, where for each \"time step\", the agent program receives a percept and returns an action. The action is then executed within the environment and the simulation loop is iterated.\n",
    "\n",
    "In the class definition below, the `step()` and `run()` methods implement the simulation loop. The simulation iterates a defined number of steps, or until the environment is considered done: a condition implemented in `is_done()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a12c517-863c-45b1-9372-d6cfcd7ee492",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.agents = set()\n",
    "    \n",
    "    def percept(self, agent):\n",
    "        \"\"\" Provides percepts for an agent \"\"\"\n",
    "        NotImplemented\n",
    "        \n",
    "    def execute_action(self, agent, action):\n",
    "        \"\"\" Executes the action for an agent \"\"\"\n",
    "        NotImplemented\n",
    "        \n",
    "    @property\n",
    "    def is_done(self):\n",
    "        \"\"\" Returns the state of the environment \"\"\"\n",
    "        NotImplemented\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\" Sends percepts and executes actions for each agent \"\"\"\n",
    "        if not self.is_done:\n",
    "            actions = {\n",
    "                agent: agent.program(self.percept(agent))\n",
    "                    for agent in self.agents}\n",
    "            for agent, action in actions.items():\n",
    "                self.execute_action(agent, action)\n",
    "            if self.is_done:\n",
    "                print(f\"Task enviroment complete. No further action.\")\n",
    "\n",
    "    def run(self, steps=10):\n",
    "        \"\"\" Simulates the environment \"\"\"\n",
    "        print(f\"Running environment simulation\")\n",
    "        for i in range(steps):\n",
    "            if self.is_done:\n",
    "                print(f\"Stopping after {i} iterations.\")\n",
    "                return\n",
    "            self.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2af9c05-a1e6-4bd1-ab8e-6b83a7d6aecf",
   "metadata": {},
   "source": [
    "We can also define a rational agent has having a program and a performance measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82797b92-322b-4342-9ed4-2539d74b6b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RationalAgent:\n",
    "    def __init__(self, program):\n",
    "        self.performance = 0\n",
    "        self.program = program"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94e8ac6-f211-49d0-853e-dc198314057b",
   "metadata": {},
   "source": [
    "### Exercise 3: Vacuum Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668e6079-f161-4fe2-a658-ebf02f221666",
   "metadata": {},
   "source": [
    "**Consider the following problem:**\n",
    "\n",
    "We have a dirty floor, made up of two locations: `0` and `1`. Each location is either dirty (`True`) or clean (`False`).\n",
    "\n",
    "We want to implement a rational agent to clean the floor.\n",
    "\n",
    "We will implement the `DirtyFloor` as an `Environment` with an initial state, `floor`. The environment will be considered \"done\" when none of the floor is dirty.\n",
    "\n",
    "The `percept` method will inform the agent whether it is dirty at its location; and the actions that can be exectuted are \"move\" and \"clean\". If a location is cleaned, it is now longer dirty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3592fa68-2a47-4839-8b26-cff1e332234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirtyFloor(Environment):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.floor = [False, True]  # initial state\n",
    "        \n",
    "    @property\n",
    "    def is_done(self):\n",
    "        \"\"\"If no tile is dirty\"\"\"\n",
    "        return not any(self.is_dirty)\n",
    "    @property\n",
    "    def is_dirty(self):\n",
    "        \"\"\" State of floor \"\"\"\n",
    "        return self.floor\n",
    "    \n",
    "    def percept(self, agent):\n",
    "        \"\"\" Tell agent if location is dirty \"\"\"\n",
    "        return self.is_dirty[agent.location]\n",
    "\n",
    "    def execute_action(self, agent, action):\n",
    "        \"\"\" Execute and react to actions by agent \"\"\"\n",
    "        match action:\n",
    "            case \"move\":\n",
    "                agent.move()\n",
    "            case \"clean\":\n",
    "                agent.clean()\n",
    "                self.is_dirty[agent.location] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9faf93-3d62-4d76-b0ff-4a8da33f6a2c",
   "metadata": {},
   "source": [
    "We also define the vacuum agent class: its starts at location `0`; improves its performance by cleaning; and can move 1 location step at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b945a93-4326-4720-b8c5-ad529e082cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vacuum(RationalAgent):\n",
    "    def __init__(self, program):\n",
    "        super().__init__(program)\n",
    "        self.location = 0\n",
    "    \n",
    "    def clean(self):  # actuator\n",
    "        self.performance += 1\n",
    "        print(f\"agent: cleaning floor at {self.location}\")\n",
    "\n",
    "    def move(self):  # actuator\n",
    "        self.location += 1\n",
    "        print(f\"agent: moved to {self.location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7427d4a1-38f9-4be3-bd69-7156c3baa018",
   "metadata": {},
   "source": [
    "**Write a simple program for a simple-reflex agent vacuum that cleans the floor if its dirty, otherwise moves.**\n",
    "\n",
    "- The `percept` will be either `True` or `False`\n",
    "- The `action` should be a string indicating the action to be executed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bb3bd4-bf5a-4f9c-aac8-47ef1afbe0b2",
   "metadata": {},
   "source": [
    "$\\textrm{program}: percept \\mapsto action$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d493ca-c464-4f3b-9706-2f1a531e86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def program(percept):\n",
    "    # write your program here\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc6106-7ef0-4d57-a752-a2051608c2a9",
   "metadata": {},
   "source": [
    "Execute the following code to run the task environment with your program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbfbea5-01a1-4f17-8e5c-556855857081",
   "metadata": {},
   "outputs": [],
   "source": [
    "environment = DirtyFloor()\n",
    "vacuum = Vacuum(program)\n",
    "\n",
    "environment.agents.add(vacuum)\n",
    "\n",
    "environment.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
